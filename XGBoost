import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pylab import mpl
import seaborn as sns
import statsmodels.api as sm
import random
from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, confusion_matrix,auc,f1_score
import sklearn.metrics
from sklearn.utils.class_weight import compute_class_weight
from sklearn.model_selection import cross_val_score,StratifiedKFold,train_test_split
from collections import Counter
import joblib
import catboost as cb
from sklearn.model_selection import GridSearchCV
from sklearn import linear_model
import catboost as cb
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import shap
from pylab import mpl
import seaborn as sns
import statsmodels.api as sm
import random
from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, confusion_matrix,auc,f1_score,roc_curve
import sklearn.metrics
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.utils.class_weight import compute_class_weight
from sklearn.model_selection import cross_val_score
from collections import Counter
import time
import joblib


inputfile1 = 'D:\\Python\\trainset.csv' 
train = pd.read_csv(inputfile1,encoding="gbk") 
inputfile2 = 'D:\\Python\\testset.csv' 
test = pd.read_csv(inputfile2,encoding="gbk") 
x_train=train.drop(columns=['Status'])
y_train=train['Status']
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline
from matplotlib import pyplot
from numpy import where
over = SMOTE(random_state=12,sampling_strategy=0.9)
under = RandomUnderSampler(sampling_strategy=0.7,random_state=123)
steps = [('u', under),('o', over) ]
pipeline = Pipeline(steps=steps)
x_train, y_train = pipeline.fit_resample(x_train, y_train)
counter = Counter(y_train)

# xgb
import xgboost as xgb
model_xgb=xgb.XGBClassifier(n_estimators=140,max_depth=3,min_child_weight=2,
                                        gamma=0.58,colsample_bytree=0.5,subsample=0.7,
                                       objective='binary:logistic',reg_alpha=0.01,
                                       nthread=4, scale_pos_weight=1, seed=27)
model_xgb.fit(x_train,y_train)
y_pred_xgb=model_xgb.predict(x_test)
print()
print('xgb accuracy:',accuracy_score(y_test,y_pred_xgb),'recall:',recall_score(y_test,y_pred_xgb),
      'f1_score:',f1_score(y_test,y_pred_xgb),'precision:',precision_score(y_test,y_pred_xgb))
print('xgb AUC为:',roc_auc_score(y_test,model_xgb.predict_proba(x_test,)[:,1]))
tn,fp,fn,tp=confusion_matrix(y_test,y_pred_xgb).ravel()
print('TN,FP,FN,TP, specificity 分别为:',tn,fp,fn,tp,tn/(tn+fp))

# roc
print('概率值：',model_xgb.predict_proba(x_test)[:,1])
fpr,tpr,thresholds=roc_curve(y_test,model_xgb.predict_proba(x_test)[:,1],pos_label=1,drop_intermediate=False)
def Find_Optimal_Cutoff(TPR, FPR, threshold):
    y = TPR - FPR
    Youden_index = np.argmax(y)  # Only the first occurrence is returned.
    optimal_threshold = threshold[Youden_index]
    point = [FPR[Youden_index], TPR[Youden_index]]
    return optimal_threshold, point
optimal_th,optimal_point=Find_Optimal_Cutoff(TPR=tpr,FPR=fpr,threshold=thresholds)
print(' Maximum Yoden index：',optimal_th)
print('optimal threshold point：',optimal_point)
def result_evaluation(y, prob, pred):
    fpr, tpr, threshold = roc_curve(y, prob[:, 1])  
    roc_auc = auc(fpr, tpr) 
    optimal_th, optimal_point = Find_Optimal_Cutoff(TPR=tpr, FPR=fpr, threshold=threshold)
    plt.figure()
    lw = 2
    plt.figure(figsize=(10, 10))
    plt.plot(fpr, tpr, color='darkorange',
             lw=lw, label='XGBoost(AUC = %0.3f)' % roc_auc) 
    plt.title('ROC Curve')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend(loc="lower right")
    plt.plot(optimal_point[0], optimal_point[1], marker='o', color='r')
    plt.text(optimal_point[0], optimal_point[1], f'Threshold:{optimal_th:.2f}')
    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.0])
    plt.show()
    # TN|FP
    # FN|TP
    pred2 = (prob[:, 1] >= optimal_th).astype(bool)
    print('Optimal subthreshold confusion matrix：')
    print(confusion_matrix(y, pred2))
    accuracy = accuracy_score(y, pred2)
    print("accuracy：", accuracy)
    print("*****************************************")
    print("*****************************************")
    print('Non-optimal subthreshold confusion matrix：')
    print(confusion_matrix(y, pred))
    accuracy = accuracy_score(y, pred)
    print("accuracy：", accuracy)
    print(tpr)

result_evaluation(y_test,model_xgb.predict_proba(x_test),model_xgb.predict(x_test))
importances_values=model_xgb.feature_importances_
importances=pd.DataFrame(importances_values,columns=["importance"])
feature_data=pd.DataFrame(x_train.columns,columns=["feature"])
importance = pd.concat([feature_data, importances], axis=1)

importance = importance.sort_values(["importance"], ascending=True)
importance["importance"] = (importance["importance"] * 1000).astype(int)
importance = importance.sort_values(["importance"])
importance.set_index('feature', inplace=True)
importance.plot.barh(color='cornflowerblue',figsize=(8, 8),title='xgboost feature importance')
plt.rcParams['font.family'] = ['sans-serif']
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False
plt.legend(loc='lower right')
plt.show()

from skimage import io,data
#shap
'''explainer=shap.TreeExplainer(model_xgb)
shap_values=explainer.shap_values(x_test)
shap_values2=explainer(x_test)
print(shap_values)
shap.summary_plot(shap_values,x_test,plot_type='bar',max_display=25)
shap.summary_plot(shap_values,x_test,max_display=25)
shap.force_plot(explainer.expected_value,
                shap_values[22,:],
                x_test.iloc[22,:],
                matplotlib=True
                )
shap.plots.scatter(shap_values2[:,"IVST"])
shap.plots.scatter(shap_values2[:,"age"])
shap.plots.scatter(shap_values2[:,"LOS"])
